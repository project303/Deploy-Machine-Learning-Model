# -*- coding: utf-8 -*-
"""0703-Sentiment_Analytics_Server.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dherrT4eI9XcqB2zP_ytyU9pkVss-Xba

# Hands-on Sentiment Analytics TF-IDF

## Library Preparation
"""

import re
import numpy as np
import pandas as pd
import pickle
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

import nltk
from nltk.corpus import stopwords
from nltk.classify import SklearnClassifier

from wordcloud import WordCloud,STOPWORDS
import matplotlib.pyplot as plt

from flask import Flask, request, jsonify

from subprocess import check_output

nltk.download('stopwords')
nltk.download('punkt_tab')

#!pip install sastrawi

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

#augment the stopwords with nonstandard twitter words
stopwords_set = set(stopwords.words("indonesian"))
stopwords_aug = {"ya","yak","iya","yg","ga","gak","gk","udh","sdh","udah","dah","nih","ini","deh","sih","dong","donk",
                 "sm","knp","utk","yaa","tdk","gini","gitu","bgt","gt","nya","kalo","cb","jg","jgn","gw","ge",
                 "sy","min","mas","mba","mbak","pak","kak","trus","trs","bs","bisa","aja","saja","no",
                 "w","g","gua","gue","emang","emg","wkwk","dr","kau","dg","gimana","apapun","apa",
                 "klo","yah","banget","pake","terus","krn","jadi","jd","mu","ku","si","hehe",
                 "tp","pa","lu","lo","lw","tw","tau","karna","kayak","ky","lg","untuk","tuk","dg","dgn"}
stopwords_all = stopwords_set.union(stopwords_aug)

def clean_text(text):
    filtered_tokens = ""
    for token in text:
      if re.search('[a-zA-Z\s]', token):
        filtered_tokens = filtered_tokens + token.lower()

    return filtered_tokens

def tokenize_clean(text):

    #tokenisasi
    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word
        in nltk.word_tokenize(sent)]

    #clean token from numeric and other character like puntuation
    filtered_tokens = []
    for token in tokens:
        if re.search('[a-zA-Z]', token):
            filtered_tokens.append(token)

    return filtered_tokens

def remove_stopwords(tokenized_text):

    cleaned_token = []
    for token in tokenized_text:
        if token not in stopwords_all:
            cleaned_token.append(token)

    return cleaned_token

def stemming_text(tokenized_text):

    #stem using Sastrawi StemmerFactory
    factory = StemmerFactory()
    stemmer = factory.create_stemmer()

    stems = []
    for token in tokenized_text:
        stems.append(stemmer.stem(token))

    return stems

def text_preprocessing(text):
    #tokenize, remove non alpha numeric and make lower
    text_tmp = re.sub('[^a-zA-Z]', ' ', text)
    text_tmp = text_tmp.lower()
    text_tmp = text_tmp.split()

    #remove stopwords
    prep02 = remove_stopwords(text_tmp)

    #stemmingnya lambat banget
    #prep03 = stemming_text(prep02)

    prep03 = ' '.join(prep02)

    return prep03

"""## Load Feature"""

with open('tfidf_vectorizer.pkl', 'rb') as file:
    loaded_tfidf = pickle.load(file)

def Predict_Sentiment(text, model):
  data_txt =[]
  data_txt.append(text_preprocessing(text))
  feature_p = loaded_tfidf.transform(data_txt)
  predict_p = model.predict(feature_p)

  return predict_p[0]

"""## Load Model"""

loaded_model = pickle.load(open('model_svm.pkl', 'rb'))

"""## Make API"""

app = Flask(__name__)

@app.route("/")
def hello():
    return "Hello World!"

@app.route('/keepalive', methods=['GET'])
def api_health():
    return jsonify(Message="Success")

@app.route("/predict", methods=["POST"])
def predict():
  content = request.json['text']
  result = Predict_Sentiment(content, loaded_model)
  return jsonify({
      'text': content,
      'result': result
  })

# Run the Flask app when this script is executed
if __name__ == "__main__":
    app.run(host='0.0.0.0', port=5000)